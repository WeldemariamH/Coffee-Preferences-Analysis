# -*- coding: utf-8 -*-
"""Coffee project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_iirXDeclCJ1k6-NuYNkdSul7bP_E8E7

**Data cleaning,exploring and preprocessing**
"""

from google.colab import files
import pandas as pd
import numpy as np

uploaded = files.upload()

file_name = list(uploaded.keys())[0]

data = pd.read_excel(file_name)

data.head()

data.shape

data.head(10)

data.tail(10)

data.describe()

data.info()

nan_values_sum = data.isna().sum()
print(nan_values_sum)

data.columns=data.columns.str.lower()

data['what is your age?'].unique()

## droping the non available values in the column age which I thought should have a values for every instance
data=data.dropna(subset=['what is your age?'])

data['what is your age?'].unique()

data.shape

nan_values_sum = data.isna().sum()
print(nan_values_sum)

##I dropped the columns with less than 50 missing values and randomly sampling and replacing the columns which have more than 100 missing values
for col in data.columns:
    missing_count=data[col].isna().sum()
    if missing_count<50:
       data=data.dropna(subset=[col],how='any')
    else:
        non_missing_values = data[col].dropna()
        random_samples = np.random.choice(non_missing_values, size=missing_count, replace=True)
        data[col][data[col].isna()] = random_samples

nan_values_sum = data.isna().sum()
print(nan_values_sum)

data.shape

data['number of children'].unique()

data['number of children'].replace('None','0',inplace=True)

data['number of children']=data['number of children'].astype(str)
data['number of children'].unique()

data['how strong do you like your coffee?'].unique()

data['how strong do you like your coffee?']=data['how strong do you like your coffee?'].replace({'Somewhat light':'Weak','Somewhat strong':'Strong'})

data['how strong do you like your coffee?'].unique()

data['gender'].value_counts()

data['gender']=data['gender'].replace("Other (please specify)","Prefer not to say")

data['ethnicity/race'].value_counts()

##I dorpped the column because majority of the samples(around 78%) are from a single ethnicity and it will be mileading to include it in the analysis
data=data.drop(['ethnicity/race'],axis=1)

data['number of children'].value_counts()

data['number of children']=data['number of children'].replace('More than 3','3')

data['number of children']=data['number of children'].replace('3','3 or more')

data['number of children'].value_counts()

data['education level'].value_counts()

data['education level']=data['education level'].replace({"Bachelor's degree":"Bachelor","Master's degree":"Masters","Some college or associate's degree":"College/Associate degree","Doctorate or professional degree":"Doctorate/Professional degree","High school graduate":"High School","Less than high school":"High School"})

"""**Featured Engineering to create new columns**"""

##creating a new column of number of cups per day and how strong it is
data['how many cups of coffee do you typically drink per day?']=data['how many cups of coffee do you typically drink per day?'].astype(str)
data["how many cups and how strong per day do you drink"]=data['how many cups of coffee do you typically drink per day?'] + ' ' + data['how strong do you like your coffee?'] + ' ' + 'cups'

##creating a new column with number of cups with the kind of coffee
data["how many cups per day and what kind do you drink"]= data['how many cups of coffee do you typically drink per day?'] + ' ' + 'cups of'+  ' ' + data['what is your favorite coffee drink?']

"""**Statistical Analysis**"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt
from collections import Counter

## creating Word cloud of the 5 columns below
text = ' '.join(data['what is your favorite coffee drink?'])
text1 = ' '.join(data['what roast level of coffee do you prefer?'])
text2= ' '.join(data['where do you typically drink coffee?'])
text3 = ' '.join(data['do you usually add anything to your coffee?'])
text4 = ' '.join(data['why do you drink coffee?'])

wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
wordcloud1 = WordCloud(width=800, height=400, background_color='white').generate(text1)
wordcloud2 = WordCloud(width=800, height=400, background_color='white').generate(text2)
wordcloud3 = WordCloud(width=800, height=400, background_color='white').generate(text3)
wordcloud4 = WordCloud(width=800, height=400, background_color='white').generate(text4)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')  # Turn off axis labels
plt.show()

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud1, interpolation='bilinear')
plt.axis('off')
plt.show()

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud2, interpolation='bilinear')
plt.axis('off')
plt.show()

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud3, interpolation='bilinear')
plt.axis('off')
plt.show()

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud4, interpolation='bilinear')
plt.axis('off')
plt.show()

##frequency distribution for the 2 columns below
sentence_counts = Counter(data['what roast level of coffee do you prefer?'])
sentence_counts1 = Counter(data['how much caffeine do you like in your coffee?'])

top_sentences = sentence_counts.most_common(5)
labels, counts = zip(*top_sentences)

plt.barh(labels, counts)
plt.xlabel('Frequency')
plt.ylabel('Sentences')
plt.title('Top Sentences by Frequency')
plt.show()

top_sentences1 = sentence_counts1.most_common(5)
labels, counts = zip(*top_sentences1)

plt.barh(labels, counts)
plt.xlabel('Frequency')
plt.ylabel('Sentences')
plt.title('Top Sentences by Frequency')
plt.show()

data.to_excel('coffee_processed_data.xlsx')
files.download('coffee_processed_data.xlsx')

print(data.columns)